{"cells":[{"cell_type":"code","source":["!pip install pyspark\n","import pyspark"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zjawp9WtVhF2","executionInfo":{"status":"ok","timestamp":1645078864712,"user_tz":-330,"elapsed":56582,"user":{"displayName":"Prasad K","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOVE6Y0fcvEiEg1Xv0vSXf40nSQo-RX7Dg4NrzaA=s64","userId":"14994654495791802840"}},"outputId":"d715f4bf-093c-4d49-f874-d7409dc7c82d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyspark\n","  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n","\u001b[K     |████████████████████████████████| 281.4 MB 27 kB/s \n","\u001b[?25hCollecting py4j==0.10.9.3\n","  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n","\u001b[K     |████████████████████████████████| 198 kB 53.2 MB/s \n","\u001b[?25hBuilding wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=04109e23b2e58bfed859be56de11b19eba4c8060963cdeeee13c97ea829c2a54\n","  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n","Successfully built pyspark\n","Installing collected packages: py4j, pyspark\n","Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"QzcSLZszVY_R","executionInfo":{"status":"ok","timestamp":1645078873340,"user_tz":-330,"elapsed":8644,"user":{"displayName":"Prasad K","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOVE6Y0fcvEiEg1Xv0vSXf40nSQo-RX7Dg4NrzaA=s64","userId":"14994654495791802840"}}},"outputs":[],"source":["# create entry points to spark\n","try:\n","    sc.stop()\n","except:\n","    pass\n","from pyspark import SparkContext, SparkConf\n","from pyspark.sql import SparkSession\n","sc=SparkContext()\n","spark = SparkSession(sparkContext=sc)"]},{"cell_type":"markdown","metadata":{"id":"BfYlRoDlVY_h"},"source":["# Convert continuous variables to categorical variables"]},{"cell_type":"markdown","metadata":{"id":"ozWyH4MJVY_o"},"source":["There are two functions we can use to split a continuous variable into categories:\n","\n","* `pyspark.ml.feature.Binarizer`: split a column of continuous features given a threshold\n","* `pyspark.ml.feature.Bucktizer`: split a column of continuous features into categories given several breaking points.\n","    + with n+1 split points, there are n categories (buckets).\n"]},{"cell_type":"markdown","metadata":{"id":"gSf8hJg4VY_q"},"source":["## Create some data"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7SjeuvF8VY_r","executionInfo":{"status":"ok","timestamp":1645078904996,"user_tz":-330,"elapsed":9227,"user":{"displayName":"Prasad K","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOVE6Y0fcvEiEg1Xv0vSXf40nSQo-RX7Dg4NrzaA=s64","userId":"14994654495791802840"}},"outputId":"5944eb4e-6b97-430c-c52f-c10cadca2720"},"outputs":[{"output_type":"stream","name":"stdout","text":["+--------------------+------------------+\n","|                  x1|                x2|\n","+--------------------+------------------+\n","| 0.47143516373249306| 6.834629351721363|\n","| -1.1909756947064645| 7.127020269829002|\n","|  1.4327069684260973|3.7025075479039495|\n","| -0.3126518960917129| 5.611961860656249|\n","| -0.7205887333650116| 5.030831653078097|\n","|  0.8871629403077386|0.1376844959068224|\n","|  0.8595884137174165| 7.728266216123741|\n","| -0.6365235044173491| 8.826411906361166|\n","|0.015696372114428918| 3.648859839013723|\n","| -2.2426849541854055| 6.153961784334937|\n","+--------------------+------------------+\n","\n"]}],"source":["import numpy as np\n","\n","import pandas as pd\n","\n","np.random.seed(seed=1234)\n","\n","pdf = pd.DataFrame({\n","        'x1': np.random.randn(10),\n","        'x2': np.random.rand(10)*10\n","    })\n","\n","np.random.seed(seed=None)\n","\n","df = spark.createDataFrame(pdf)\n","\n","df.show()"]},{"cell_type":"markdown","metadata":{"id":"VNWs3Z6PVY_v"},"source":["## Binarize the column x1 and Bucketize the column x2"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9sHFIiw1VY_w","executionInfo":{"status":"ok","timestamp":1645079122338,"user_tz":-330,"elapsed":396,"user":{"displayName":"Prasad K","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOVE6Y0fcvEiEg1Xv0vSXf40nSQo-RX7Dg4NrzaA=s64","userId":"14994654495791802840"}},"outputId":"431999fd-8a44-41c5-d44d-1478b3460159"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Binarizer_4f753e50f033"]},"metadata":{},"execution_count":8}],"source":["from pyspark.ml.feature import Binarizer, Bucketizer\n","\n","# threshold = 0 for binarizer\n","binarizer = Binarizer(threshold=0, \n","                      inputCol='x1', \n","                      outputCol='x1_new')\n","\n","binarizer"]},{"cell_type":"code","execution_count":9,"metadata":{"collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"OWx8q-xwVY_z","executionInfo":{"status":"ok","timestamp":1645079144475,"user_tz":-330,"elapsed":8,"user":{"displayName":"Prasad K","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOVE6Y0fcvEiEg1Xv0vSXf40nSQo-RX7Dg4NrzaA=s64","userId":"14994654495791802840"}},"outputId":"670209ad-cc8f-446b-90a1-3238e6c3f9d2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Bucketizer_c5385bc64c40\n"]}],"source":["#######################################################################\n","# provide 5 split points to generate 4 buckets\n","bucketizer = Bucketizer(splits=[0, 2.5, 5, 7.5, 10], \n","                        inputCol='x2', \n","                        outputCol='x2_new')\n","print(bucketizer)"]},{"cell_type":"code","source":["# pipeline stages\n","from pyspark.ml import Pipeline\n","stages = [binarizer, bucketizer]\n","pipeline = Pipeline(stages=stages)\n","\n","#################################################################\n","# fit the pipeline model and transform the data\n","pipeline.fit(df).transform(df).show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_FiKSYSlW6Ih","executionInfo":{"status":"ok","timestamp":1645079162149,"user_tz":-330,"elapsed":1171,"user":{"displayName":"Prasad K","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOVE6Y0fcvEiEg1Xv0vSXf40nSQo-RX7Dg4NrzaA=s64","userId":"14994654495791802840"}},"outputId":"ca28e41d-eb07-4bc6-95f3-6457fd19d017"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["+--------------------+------------------+------+------+\n","|                  x1|                x2|x1_new|x2_new|\n","+--------------------+------------------+------+------+\n","| 0.47143516373249306| 6.834629351721363|   1.0|   2.0|\n","| -1.1909756947064645| 7.127020269829002|   0.0|   2.0|\n","|  1.4327069684260973|3.7025075479039495|   1.0|   1.0|\n","| -0.3126518960917129| 5.611961860656249|   0.0|   2.0|\n","| -0.7205887333650116| 5.030831653078097|   0.0|   2.0|\n","|  0.8871629403077386|0.1376844959068224|   1.0|   0.0|\n","|  0.8595884137174165| 7.728266216123741|   1.0|   3.0|\n","| -0.6365235044173491| 8.826411906361166|   0.0|   3.0|\n","|0.015696372114428918| 3.648859839013723|   1.0|   1.0|\n","| -2.2426849541854055| 6.153961784334937|   0.0|   2.0|\n","+--------------------+------------------+------+------+\n","\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"1UTI2t4sW-MS"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"2.3-continuous-variable-to-categorical-variable.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}